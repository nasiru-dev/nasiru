{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15451b38-c184-45b9-b0b2-b38d42a7ae5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '–' (U+2013) (2016723218.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    Ejaztech.AI – Supervised Machine Learning: Regression & Classification\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '–' (U+2013)\n"
     ]
    }
   ],
   "source": [
    "# Phase 2 – Project 2: Logistic Regression (Cancer Classification)\n",
    "\n",
    "Ejaztech.AI – Supervised Machine Learning: Regression & Classification\n",
    "\n",
    "Objective: Develop and evaluate a Logistic Regression model to classify cancer cases using a real-world dataset.\n",
    "\n",
    "Dataset: https://www.kaggle.com/datasets/erdemtaha/cancer-data/data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8e266-9654-4320-9433-537f4031ccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459a10b-98c3-4a5d-988b-92b229a0de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"cancer_data.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cf587-ea05-4f68-a340-23789cd678a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b27b1-a89a-41ea-a965-c461b54653ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['diagnosis'].value_counts())\n",
    "\n",
    "sns.countplot(x='diagnosis', data=df)\n",
    "plt.title(\"Class distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c00dce-ea14-4e21-aa41-3a4e07ed0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "df.isna().sum()\n",
    "\n",
    "# Drop rows with missing values \n",
    "df = df.dropna()\n",
    "\n",
    "# Drop non-informative ID column if present\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop(columns=['id'])\n",
    "\n",
    "print(\"Unique diagnosis values:\", df['diagnosis'].unique())\n",
    "\n",
    "mapping = {'M': 1, 'B': 0}\n",
    "df['diagnosis'] = df['diagnosis'].map(mapping)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97fc71-94da-4185-9341-817cbbd2f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['diagnosis'])\n",
    "y = df['diagnosis']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e02afa-d8ef-43a2-8932-5361d8e9beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a7299-b661-4064-b7c9-59da8248460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coefficient\": log_reg.coef_[0]\n",
    "}).sort_values(by=\"coefficient\", ascending=False)\n",
    "\n",
    "coefficients.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ae21a-5b71-4742-a358-564b9c03c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = log_reg.predict(X_train_scaled)\n",
    "y_test_pred = log_reg.predict(X_test_scaled)\n",
    "y_test_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "def print_metrics(y_true, y_pred, dataset_name=\"\"):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"{dataset_name} Accuracy : {acc:.4f}\")\n",
    "    print(f\"{dataset_name} Precision: {prec:.4f}\")\n",
    "    print(f\"{dataset_name} Recall   : {rec:.4f}\")\n",
    "    print(f\"{dataset_name} F1-score : {f1:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print_metrics(y_train, y_train_pred, \"Train\")\n",
    "print_metrics(y_test, y_test_pred, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc2e75-61a8-4214-9397-e1ca0c5a8ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36eb63-2a4c-4e28-a674-d2d3515c4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35083603-4f7c-4be2-b522-b3458eefdc1a",
   "metadata": {},
   "source": [
    "## 10. Interpretation and insights\n",
    "\n",
    "In this project, I used Logistic Regression to classify cancer cases into two classes based on multiple medical measurements. The main evaluation metrics I looked at were accuracy, precision, recall, F1-score, and AUC.\n",
    "\n",
    "Accuracy tells me the overall proportion of correct predictions out of all samples. Precision focuses on how many of the cases predicted as positive (cancer) are actually positive, which is important to avoid too many false alarms. Recall measures how many of the actual positive cases the model correctly catches, which is critical in healthcare because missing a real cancer case can be very dangerous.\n",
    "\n",
    "The confusion matrix helps me see the counts of true positives, true negatives, false positives, and false negatives. By inspecting it, I can understand whether the model is making more false positives (predicting cancer when it is not present) or more false negatives (predicting no cancer when it is actually present). In a cancer detection setting, false negatives are usually more serious, so a high recall is especially valuable even if it slightly reduces precision.\n",
    "\n",
    "Looking at the model coefficients, I can see which features contribute most positively or negatively to predicting the cancer class. Features with large positive coefficients increase the probability of the positive class when they increase, while features with large negative coefficients decrease that probability. This gives some interpretability and hints about which measurements are more influential for the model's decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a3fd34-a766-405a-818b-9f2e419783ca",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "The goal of this project was to build a Logistic Regression model to classify cancer cases using the Ejaztech.AI Phase 2 dataset and to practice supervised machine learning for classification. I followed a complete workflow: data loading, cleaning and preprocessing, encoding the target variable, feature scaling, model training, and model evaluation using several metrics.\n",
    "\n",
    "The model achieved a reasonable performance in terms of accuracy, precision, recall, F1-score, and AUC, showing that Logistic Regression can be an effective baseline for this medical classification problem. By examining the confusion matrix and the balance between precision and recall, I gained insight into the trade-off between false positives and false negatives, which is especially important in cancer detection.\n",
    "\n",
    "Overall, this project strengthened my understanding of how Logistic Regression works, how to interpret its coefficients, and how to evaluate a classification model in a healthcare context. Possible next steps to improve performance include trying other algorithms (such as Random Forests or Gradient Boosting), performing more advanced feature engineering, and tuning hyperparameters to better balance recall and precision for medical use cases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
